<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>分布式文件系统-HDFS</title>
    <url>/2020/01/30/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E2%80%94HDFS/</url>
    <content><![CDATA[<h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>仅用于个人学习使用且<strong>电脑配置有限</strong>，因此搭建伪分布式模式，即在单节点上启动NameNode、DataNode、SecondaryNameNode等进程，但操作、原理与完全分布式模式一样仅学习还是够用的。HDFS作为Hadoop两大重要组成部分之一，主要解决海量数据存储问题，<a href="https://baike.baidu.com/item/HDFS" target="_blank" rel="noopener">详细信息点击这里哟！</a><br>下面是我的实验环境（Hadoop大多运行在Linux系统下）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">服务器：Centos 6</span><br><span class="line">虚拟机：VMware Workstation Pro 13.0</span><br><span class="line">SSH：MobaXterm</span><br><span class="line">PC：Win10</span><br></pre></td></tr></table></figure>



<h1 id="二、HDFS运行原理"><a href="#二、HDFS运行原理" class="headerlink" title="二、HDFS运行原理"></a>二、HDFS运行原理</h1><h2 id="1-HDFS概念"><a href="#1-HDFS概念" class="headerlink" title="1.HDFS概念"></a>1.HDFS概念</h2><ul>
<li>HDFS，是 Hadoop Distribute File System 的简称，意为：Hadoop 分布式文件系统。是 Hadoop 核心组件之一。 </li>
<li>HDFS解决的问题就是大数据的存储。它们是横跨在多台计算机上的存储系统。分布式文件系统在大数据时代有着广泛的应用前景，它们为存储和处理超大规模数据提供所需的扩展能力。</li>
<li>HDFS起源于Google的GFS论文（GFS，Mapreduce，BigTable为google的旧的三驾马车）</li>
</ul>
<h2 id="2-HDFS优缺点"><a href="#2-HDFS优缺点" class="headerlink" title="2.HDFS优缺点"></a>2.HDFS优缺点</h2><ul>
<li><p>优点</p>
<ul>
<li>高容错性</li>
<li>适合大数据处理</li>
<li>可搭建在廉价的机器上</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>不适合低延迟数据访问</li>
<li>不适合大量小文件进行存储</li>
<li>不支持文件并发写入和文件随机修改</li>
</ul>
</li>
</ul>
<h2 id="3-HDFS组成架构（重点）"><a href="#3-HDFS组成架构（重点）" class="headerlink" title="3.HDFS组成架构（重点）"></a>3.HDFS组成架构（重点）</h2><p>HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。</p>
<p><img src="https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsarchitecture.gif" alt="HDFS架构设计"></p>
<ul>
<li><p>Client：客户端</p>
<ul>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block</li>
<li>与NameNode交互，获取文件的位置信息  </li>
<li>与DataNode交互，读取或者写入数据  </li>
<li>提供一些命令来管理HDFS</li>
</ul>
</li>
<li><p>NameNode：名称节点。就是Master，它是一个主管、管理者、老大  </p>
<ul>
<li>存储元数据（内存中）</li>
<li>保存block与DataNode之间的映射关系</li>
</ul>
</li>
<li><p>DataNode：数据节点。就是小弟。NameNode下达命令，DataNode执行实际的操作  </p>
<ul>
<li>存储文件内容（磁盘）</li>
<li>维护block ID到DataNode本地文件的映射关系</li>
<li>存储到本地Linux系统中</li>
</ul>
</li>
<li><p>SecondaryNameNode：第二名称节点。NameNode的<strong>冷备份</strong></p>
<ul>
<li>主要解决EditLog不断增大的问题（下面会详细介绍）</li>
</ul>
</li>
</ul>
<h3 id="3-1-Block大小"><a href="#3-1-Block大小" class="headerlink" title="3.1 Block大小"></a>3.1 Block大小</h3><p>​        HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数(<code>dfs.blocksize</code>)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。HDFS的块设计成，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。</p>
<h3 id="3-2-副本机制"><a href="#3-2-副本机制" class="headerlink" title="3.2 副本机制"></a>3.2 副本机制</h3><p>​        为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目，副本系数可以在文件创建的时候指定，也可以在之后改变。  </p>
<h3 id="3-3-元数据"><a href="#3-3-元数据" class="headerlink" title="3.3 元数据"></a>3.3 元数据</h3><p>​         数据的描述信息，比如这个文件的路径，这个文件的副本数，每个副本保存在哪个DataNode上，就相当于真真正正的一个人，他有一些信息：身高、体重、姓名、性别等，通过这些信息就可以描述这个人，那描述HDFS上真实数据的信息，就称为元数据。  <img src="https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsdatanodes.gif" alt=""></p>
<h2 id="4-HDFS运行原理（重点）"><a href="#4-HDFS运行原理（重点）" class="headerlink" title="4.HDFS运行原理（重点）"></a>4.HDFS运行原理（重点）</h2><ul>
<li><p>NameNode启动</p>
<ul>
<li>将FsImage文件内容加载到内存中</li>
<li>执行EditLog文件的各项操作，使得内存的元数据和实际的同步</li>
<li>得到新的FsImage并创建一个空的EditLog文件</li>
<li>启动后所有的操作都记录在EditLog，并不会添加在FsImage中</li>
</ul>
</li>
<li><p>DataNode启动</p>
</li>
<li><p>SecondaryNameNode启动</p>
</li>
</ul>
<p>FsImage：维护文件系统树以及文件树中所有的文件和文件夹的元数据，但不维护块在哪个DataNode（通过NameNode与DataNode实时沟通生成文件目录并储存在内存中），主要记录文件复制等级、修改和访问时间、访问权限、块大小等</p>
<p>EditLog：操作日志文件，记录所有针对文件的增删改查等操作</p>
<p>通过FsImage和EditLog的结合就能完成所有文件的增删改查，但随着文件的修改EditLog会变得非常庞大，这时候就需要通过SecondaryNameNode来解决。</p>
<h3 id="4-1-NameNode运行机制"><a href="#4-1-NameNode运行机制" class="headerlink" title="4.1 NameNode运行机制"></a>4.1 NameNode运行机制</h3><ul>
<li>1.SecondaryNameNode会定期和NameNode通信，请求暂停使用EditLog文件，暂时将新的操作写在新的文件EditLog.new上且瞬间完成。</li>
<li>2.SecondaryNameNode通过HTTP GET方式从NameNode上获取FsImage和EditLog并下载到本地。</li>
<li>3.SecondaryNameNode将下载的FsImage载入内存并与EditLog进行合并</li>
<li>4.SecondaryNameNode通过post方式将新的FsImage发送给NameNode  </li>
<li>5.NameNode将新的FsImage替换掉，同时将EditLog.new替换成EditLog  </li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/1.png" alt=""></p>
<h3 id="4-2-HDFS思维导图"><a href="#4-2-HDFS思维导图" class="headerlink" title="4.2 HDFS思维导图"></a>4.2 HDFS思维导图</h3><p>凑合着看，别问为什么是个图片，问就是没钱^_^</p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/2.jpg" alt=""></p>
<h1 id="三、HDFS实操"><a href="#三、HDFS实操" class="headerlink" title="三、HDFS实操"></a>三、HDFS实操</h1><h2 id="1-安装虚拟机"><a href="#1-安装虚拟机" class="headerlink" title="1.安装虚拟机"></a>1.安装虚拟机</h2><p>这里强烈推荐一个微信公众号<strong>管家课堂</strong>，为数不多的超良心公众号，各种常用软件安装教程。<a href="https://mp.weixin.qq.com/s/GPMsgE-BFklqUmFwafPVLw" target="_blank" rel="noopener">这是一个安装传送门，记得关注哟</a></p>
<h2 id="2-安装Linux并安装Hadoop"><a href="#2-安装Linux并安装Hadoop" class="headerlink" title="2.安装Linux并安装Hadoop"></a>2.安装Linux并安装Hadoop</h2><p>我已经将配置好的Centos6打包好了<a href="https://pan.baidu.com/s/1eJx9wWaxsr4n_68m2GcD1w" target="_blank" rel="noopener">网盘链接，提取码:cpj1</a></p>
<p>[当然爱折腾的可自行下载、配置、安装]。</p>
<p>解压下载好的安装包，打开VMware虚拟机，点击文件-&gt;打开（<strong>快捷键Ctrl + O</strong>），找到刚才解压的路径打开linux01虚拟机配置即可</p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/3.png" alt=""></p>
<p>出现该提示点击<strong>我已移动该虚拟机</strong>，无需其他操作等待加载完成即可</p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/4.png" alt=""></p>
<p>该虚拟机用户名如下，要使用<strong>root账户</strong>[权限高]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">用户1：admin</span><br><span class="line">密码：123456</span><br><span class="line">用户2：root</span><br><span class="line">密码：123456</span><br></pre></td></tr></table></figure>

<p>如下即为Centos6的桌面</p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/5.png" alt=""></p>
<p>从PC机文件上传到虚拟机并不容易，因此我们使用SSH远程连接Linux，这里我使用MobaXterm。首先我们需要<strong>获取这个Linux的IP地址</strong>，右击桌面点击在终端打开输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/6.png" alt=""></p>
<p>框选出来的就是本机IP，打开<strong>MobaXterm-&gt;Session-&gt;SSH</strong>输入刚才得到的IP即可，</p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/7.png" alt=""></p>
<p>第一次连接需要输入root用户的密码，<strong>注：Linux为保护用户安全，在你输入密码时屏幕并不会出现任何符号。</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/8.png" alt=""></p>
<p>黑色背景区域即为Linux终端，一般使用Lin系统是不会选择启动图形化界面的。左边文件夹区域为Linux的文件系统，当需要将PC机文件上传到Linux时，只需拖拽即可。</p>
<p><strong>这个Linux系统已经集成了Hadoop，不需要额外安装。</strong></p>
<h2 id="3-HDFS常见操作命令"><a href="#3-HDFS常见操作命令" class="headerlink" title="3.HDFS常见操作命令"></a>3.HDFS常见操作命令</h2><p>首先需要修改一下配置，输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;hosts</span><br></pre></td></tr></table></figure>

<p>按<code>i</code>，将<strong>最后一行的IP修改为自己</strong>的，按<code>Esc</code>退出修改模式并输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:wq</span><br></pre></td></tr></table></figure>

<p>保存并关闭，上述只是Linux系统的操作命令，在我另一篇文章会有详细介绍，并且Hadoop的一些命令和Linux及其一致。</p>
<p>启动HDFS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>启动完成可查看当前进程检查HDFS是否全部启动，输入如下代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>当看到NameNode、DataNode、SecondaryNameNode即为启动成功</p>
<p><img src="https://cdn.jsdelivr.net/gh/dashingwj/BlogImg/HDFS/9.png" alt=""></p>
<p>若有没有启动的可以结束HDFS并在此启动，代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">stop-dfs.sh</span><br><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>查看当前HDFS文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -ls &#x2F;</span><br></pre></td></tr></table></figure>

<p>在HDFS新建文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir &#x2F;Test	&#x2F;&#x2F;在根目录下创建Test文件夹</span><br></pre></td></tr></table></figure>

<p>删除HDFS指定文件(夹)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -rm -f &#x2F;Test	&#x2F;&#x2F;删除指定文件夹</span><br><span class="line">hdfs dfs -rm &#x2F;name.txt	&#x2F;&#x2F;删除指定文件</span><br></pre></td></tr></table></figure>

<p>将Linux文件上传到HDFS上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -put &#x2F;MapReduce&#x2F;test.txt &#x2F;Data</span><br></pre></td></tr></table></figure>

<p>第一个路径为Linux要上传文件的绝对路径，第二个为上传到HDFS的路径（上述代码不要直接复制，因为你的Linux和HDFS可能没有这个文件夹和文件）</p>
<p>将HDFS文件下载到Linux本地</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -get &#x2F;Data&#x2F;text &#x2F;MapReduce</span><br></pre></td></tr></table></figure>

<p>同理第一个路径为HDFS要下载文件的绝对路径，第二个为下载到Linux本地的文件路径（Linux本地不能有与要下载文件同名的文件，道理很显然）</p>
<p>最后，关机命令（虚拟机上的系统很脆弱，要小心呵护不要暴力关机！！！）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">shutdown -h now</span><br></pre></td></tr></table></figure>

<p>上述命令基本够用学习MapReduce了，期待下一遍博文将会详细介绍Hadoop另一个重要组件<strong>MapReduce</strong></p>
<center>![](https://bpic.588ku.com//original_origin_min_pic/18/06/20/f651302009e2e93fb59702e8100e9817.jpg)</center>
- 需要MobaXterm及其他技术支持联系微信^_^ skr~]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>悦读</tag>
      </tags>
  </entry>
</search>
